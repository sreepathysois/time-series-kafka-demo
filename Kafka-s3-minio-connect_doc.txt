 ########### Kafka Commands and Kafka S3/Minio Connect Set-up Details############


############Kafka S3/Minio Set-Up########################

su -l kafka
mkdir confluent-plugins
cd confluent-plugins
wget https://api.hub.confluent.io/api/plugins/confluentinc/kafka-connect-s3/versions/5.2.0/archive
unzip archive
cd ..
mkdir plugins
mkdir plugins/kafka-connect-s3
mv confluent-plugins/confluentinc-kafka-connect-s3-5.2.0/lib/* plugins/kafka-connect-s3/


cd ~/plugins
touch connector.properties

# Kafka broker IP addresses to connect to
bootstrap.servers=localhost:9092
# Path to directory containing the connector jar and dependencies
plugin.path=/home/kafka/plugins
# Converters to use to convert keys and values
key.converter=org.apache.kafka.connect.storage.StringConverter
value.converter=org.apache.kafka.connect.storage.StringConverter
# The internal converters Kafka Connect uses for storing offset and configuration data
internal.key.converter=org.apache.kafka.connect.json.JsonConverter
internal.value.converter=org.apache.kafka.connect.json.JsonConverter
internal.key.converter.schemas.enable=false
internal.value.converter.schemas.enable=false
offset.storage.file.filename=/tmp/connect.offsets


touch s3-sink.properties

name=s3-sink
connector.class=io.confluent.connect.s3.S3SinkConnector
tasks.max=1
topics=minio_topic
s3.region=us-east-1
s3.bucket.name=kafka-bucket
s3.part.size=5242880
flush.size=3
store.url=http://127.0.0.1:9000
storage.class=io.confluent.connect.s3.storage.S3Storage
format.class=io.confluent.connect.s3.format.json.JsonFormat
schema.generator.class=io.confluent.connect.storage.hive.schema.DefaultSchemaGenerator
partitioner.class=io.confluent.connect.storage.partitioner.DefaultPartitioner
schema.compatibility=NONE



su -l kafka
mkdir .aws
vi credentials
[default]
aws_access_key_id = minio
aws_secret_access_key = minio123

#####start the connector command ######


su -l kafka
cd kafka
./bin/connect-standalone.sh ../plugins/connector.properties ../plugins/s3-sink.properties

######################################


#####Create Kafka Topic Command##########

~/kafka/bin/kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic minio_topic

###############


####Kafka Sample Produce Data to topic############

echo "Hello, World" | ~/kafka/bin/kafka-console-producer.sh --broker-list localhost:9092 --topic minio_topic > /dev/null
echo "Hello, World" | ~/kafka/bin/kafka-console-producer.sh --broker-list localhost:9092 --topic minio_topic > /dev/null
echo "Hello, World" | ~/kafka/bin/kafka-console-producer.sh --broker-list localhost:9092 --topic minio_topic > /dev/null

Consume Data from Kafka Topic 

~/kafka/bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic TutorialTopic --from-beginning

################################


##########Kafka Time Series Data Produce Data Command#############

python bin/sendStream.py data/data.csv my-stream

python bin/sendStream.py data/data.csv my-stream --speed 10

docker run -it --rm \
      -v $PWD:/home \
      --network=host \
      kafkacsv python bin/sendStream.py data/data.csv my-stream

################################

##########Kafka Time Series Data Consume  Data Command#############

python bin/processStream.py my-stream
or with Docker:

docker run -it --rm \
      -v $PWD:/home \
      --network=host \
      kafkacsv python bin/processStream.py my-stream

################################


############Kafka and Zookeeeper Start and List Topics Command ##############

/home/kafka/kafka/bin/zookeeper-server-start.sh /home/kafka/kafka/config/zookeeper.properties

/bin/sh -c /home/kafka/kafka/bin/kafka-server-start.sh /home/kafka/kafka/config/server.properties

bin/kafka-topics.sh --list --zookeeper localhost:2181

./bin/kafka-topics.sh --bootstrap-server=localhost:9092 --describe --topic minio_topic

sed -i -r 's/name=.*/name=pathy/' /home/kafka/plugins/s3-sink.properties
